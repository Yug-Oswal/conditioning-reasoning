{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d0652e-5bfd-4b11-91fb-9948cbdbdd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import LogitsProcessorList, RepetitionPenaltyLogitsProcessor\n",
    "\n",
    "from peft import get_peft_model, LoraConfig, PeftModel\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af99d86-1219-4e14-8007-13a6299e9829",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.accelerator.current_accelerator().type if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../models/deepseek/DeepSeek-R1-Distill-Qwen-1.5B/\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"../models/deepseek/DeepSeek-R1-Distill-Qwen-1.5B/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6857b42e-03bc-4e70-97f4-b5f550bc7489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v3 load\n",
    "model = PeftModel.from_pretrained(model, \"../models/own/cr_v3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b39b622-ca19-4d3d-a824-adb44f9b9b60",
   "metadata": {},
   "source": [
    "## Conditioning section: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0349bea5-a644-4d33-baf5-343dc427601c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"Elfsong/BBQ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc44ca4-8af3-47e5-af23-d3c9d1f73be5",
   "metadata": {},
   "source": [
    "### Utilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ea0b4e-7ab9-4603-80bd-5be7f4e8129c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(i=0):\n",
    "    i = i\n",
    "    input_text = \"Context: \" + ds['religion'][i]['context'] + \\\n",
    "    \"\\nQuestion: \" + ds['religion'][i]['question'] + \"\\nChoices: \" + ds['religion'][i]['ans0'] + \\\n",
    "    \", \" + ds['religion'][i]['ans1'] + \", \" + ds['religion'][i]['ans2'] + \"\\n\\n<think>\\n\"\n",
    "    print(input_text)\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a675364c-431b-441d-98db-3b6aa60c4108",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "r=16,\n",
    "lora_alpha=8,\n",
    "lora_dropout=0.05,\n",
    "bias=\"none\",\n",
    "task_type=\"CAUSAL_LM\")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.to(device)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a65a8b-cc06-4e9d-8ef3-c9ab73283729",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_infer(model, inputs, max_new_tokens=600):\n",
    "    max_new_tokens = 600\n",
    "    generated_ids = inputs.input_ids.clone().to(model.device)\n",
    "    past_key_values = None\n",
    "    \n",
    "    processors = LogitsProcessorList()\n",
    "    processors.append(RepetitionPenaltyLogitsProcessor(penalty=1.2))\n",
    "    \n",
    "    for step in range(max_new_tokens):\n",
    "        next_input_ids = generated_ids[:, -1:] if past_key_values else generated_ids\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=next_input_ids, past_key_values=past_key_values, use_cache=True)\n",
    "            logits = outputs.logits  \n",
    "            past_key_values = outputs.past_key_values\n",
    "    \n",
    "        next_token_logits = logits[:, -1, :]  \n",
    "        next_token_logits = processors(generated_ids, next_token_logits)\n",
    "        next_token_id = torch.argmax(next_token_logits, dim=-1, keepdim=True) \n",
    "    \n",
    "        generated_ids = torch.cat([generated_ids, next_token_id], dim=-1)\n",
    "    \n",
    "        if next_token_id.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    print(\"Generated Text:\\n\", generated_text)\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645f5f22-91f3-49ad-8c4b-59d0681e43e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def condition_model(model, tid_reasoning, tid_output):\n",
    "    max_new_tokens = 600\n",
    "    generated_ids = inputs.input_ids.clone().to(model.device)\n",
    "    past_key_values = None\n",
    "    \n",
    "    processors = LogitsProcessorList()\n",
    "    processors.append(RepetitionPenaltyLogitsProcessor(penalty=1.2))\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "    kl_loss = nn.KLDivLoss(reduction='batchmean')  \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step in range(max_new_tokens):\n",
    "            next_input_ids = generated_ids[:, -1:] if past_key_values else generated_ids\n",
    "        \n",
    "            outputs = model(input_ids=next_input_ids, past_key_values=past_key_values, use_cache=True)\n",
    "            past_key_values = outputs.past_key_values\n",
    "\n",
    "            next_token_id = torch.argmax(processors(generated_ids, outputs.logits[:, -1, :]), dim=-1, keepdim=True) \n",
    "            generated_ids = torch.cat([generated_ids, next_token_id], dim=1)\n",
    "        \n",
    "            if next_token_id.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "    model.train()\n",
    "    outputs = model(generated_ids)\n",
    "    logits = outputs.logits.squeeze(0)  \n",
    "\n",
    "    reasoning_logits = []\n",
    "    for tid in tid_reasoning:\n",
    "        if tid < logits.size(0):\n",
    "            reasoning_logits.append(F.log_softmax(processors(generated_ids[:, :tid], logits[tid].unsqueeze(0)), dim=-1))\n",
    "    if not reasoning_logits:\n",
    "        print(\"No valid reasoning token positions.\")\n",
    "        return\n",
    "    avg_reasoning_logits = torch.stack(reasoning_logits).mean(dim=0) \n",
    "    \n",
    "    output_logits = []\n",
    "    for tid in tid_output:\n",
    "        if tid < logits.size(0):\n",
    "            output_logits.append(F.softmax(processors(generated_ids[:, :tid], logits[tid].unsqueeze(0)), dim=-1))\n",
    "    if not output_logits:\n",
    "        print(\"No valid output token positions.\")\n",
    "        return\n",
    "    avg_output_logits = torch.stack(output_logits).mean(dim=0)  \n",
    "\n",
    "    loss = kl_loss(avg_reasoning_logits, avg_output_logits)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77dc82d-3747-4560-9f70-94f8b50dfcdb",
   "metadata": {},
   "source": [
    "## Evaluating section:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39159d40-6e36-44e6-a9c3-b68e5071ade8",
   "metadata": {},
   "source": [
    "### Utilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd585a92-bdd7-4e31-af88-63c5cda15efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(i=0):\n",
    "    i = i\n",
    "    input_text = \"Context: \" + ds['religion'][i]['context'] + \\\n",
    "    \"\\nQuestion: \" + ds['religion'][i]['question'] + \"\\nChoices: \" + ds['religion'][i]['ans0'] + \\\n",
    "    \", \" + ds['religion'][i]['ans1'] + \", \" + ds['religion'][i]['ans2'] + \"\\n\\n<think>\\n\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eca864b-39a4-4642-a58b-7a6a5cf66706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_infer(model, inputs, max_new_tokens=600):\n",
    "    max_new_tokens = 600\n",
    "    generated_ids = inputs.input_ids.clone().to(model.device)\n",
    "    print(\"Input:\\n\", tokenizer.decode(generated_ids.squeeze(0)))\n",
    "    past_key_values = None\n",
    "    \n",
    "    processors = LogitsProcessorList()\n",
    "    processors.append(RepetitionPenaltyLogitsProcessor(penalty=1.2))\n",
    "    \n",
    "    for step in range(max_new_tokens):\n",
    "        next_input_ids = generated_ids[:, -1:] if past_key_values else generated_ids\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=next_input_ids, past_key_values=past_key_values, use_cache=True)\n",
    "            logits = outputs.logits  \n",
    "            past_key_values = outputs.past_key_values\n",
    "    \n",
    "        next_token_logits = logits[:, -1, :]  \n",
    "        next_token_logits = processors(generated_ids, next_token_logits)\n",
    "        next_token_id = torch.argmax(next_token_logits, dim=-1, keepdim=True) \n",
    "    \n",
    "        generated_ids = torch.cat([generated_ids, next_token_id], dim=-1)\n",
    "    \n",
    "        if next_token_id.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    print(generated_text[generated_text.find(\"</think>\") if generated_text.find(\"</think>\") != -1 else -150:].strip())\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe3e413-2a29-4fd2-ad3f-10ec6c528705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, samples=50):\n",
    "    for i in range(samples):\n",
    "        print(\"-------------------------\\nActual label: \", ds['religion'][i]['ans' + str(ds['religion'][i]['answer_label'])])\n",
    "        check_infer(model, inputs=load_data(i=i))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch)",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
